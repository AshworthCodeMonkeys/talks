GNU Parallel
============

Motivation: Use all the extra cores that are lying around

>"Spend an hour walking through the tutorial. Your command line will love you for it."

>"The tutorial is not to show realistic examples from the real world."

- https://www.gnu.org/software/parallel/man.html  
- https://www.gnu.org/software/parallel/parallel_tutorial.html  
- https://www.biostars.org/p/63816/

Here are two examples to show what it looks like. Don't worry about the code for now. Just look at top or htop

1. Counting all the lines in a bunch of fastq files

        parallel "echo {}; zcat {} | wc -l" ::: *fastq.gz    

2. Running blast in parallel

        cat query.fasta \
        | parallel --pipe --block 1M --recstart ">" \
            "blastn -query - -db target.fasta -evalue 1e-10 -outfmt 6" \
        >blastoutput.txt

Broadly these two examples can be described as:

1. Do the same thing for lots of files or inputs
2. Process a single large file by splitting it up into smaller chunks

Do the same thing for lots of files or inputs
---------------------------------------------

Typically you might write a script or a for loop on the command line:

    for f in *.fastq.gz
    do
        echo $f;
        zcat $f | wc -l;
    done

Notice how the $f gets replaced by the placeholder {} in this:

    parallel "echo {}; zcat {} | wc -l" ::: *fastq.gz
    
The oddest bit of this to me when I first started was ":::". So let's see how that works. Think of it as a way of delimiting the parallel command from the arguments (am guessing Ole Tange used ::: because it was so unusual. But there is an option to change it if it really bothers you.

Anything after the ::: is an argument that will get put in the placeholder {}.

Where this quickly becomes more powerful than a for-loop is in the parallelising. To parallelise in for, you would need to put these jobs in the background but then you wouldn't know when they were done or done correctly. And you have no control over how well you can parallelise. So if you did this:

    for f in *.fastq.gz
    do
        echo $f;
        zcat $f | wc -l &
    done

You'd quickly have a problem if there were 1000 fastq.gz files and only 32 cores on your computer.

With parallel, you only run as many jobs in parallel as there are cores available. Or you can ask for less cores, if you're feeling nice to fellow users:

    parallel -j 8 "echo {}; zcat {} | wc -l" ::: *fastq.gz

Because it is in parallel, some files may finish before others. But let's say you want the output in the same order, you should use -k:

    parallel -k -j 8 "echo {}; zcat {} | wc -l" ::: *fastq.gz

The next cool thing is that you can have two or more sets of arguments and parallel will operate on all possible combinations:

    parallel "echo {}"      ::: A B C
    parallel "echo {1} {2}" ::: A B C ::: 4 5
    parallel "echo {2} {1}" ::: A B C ::: 4 5

As you can imagine this is really good when you want to do all vs all blasts for let's say 5 species. From now on we're going to use the --dry-run option. First you want to make blastdbs for 5 files:

    parallel --dry-run "makeblastdb -dbtype prot -in {}" ::: *.fa
    
    parallel --dry-run "blastp -query {1} -db {2} -out {1}.{2}.blast" ::: *.fa ::: *.fa

You can also do clever things with the placeholders. Sometimes you don't want extensions - eg in this case you want it to be called species1.species2.blast and not species1.fa.species2.fa.blast

    parallel --dry-run "blastp -query {1} -db {2} -out {1.}.{2.}.blast" ::: *.fa ::: *.fa

Similarly, you can also format the placeholders to have the full directory path or not.

Example:

    parallel --dry-run "blastp -query {1} -db {2} -out {1/.}.{2/.}.blast" ::: input/*.fa ::: input/*.fa

Instead of providing arguments with ::: you can also provide a file with the arguments using ::::

    parallel --dry-run "blastp -query {1} -db {2} -out {1/.}.{2/.}.blast" :::: input_files.txt :::: input_files.txt

What if you don't want all combinations, but only a specific set of input parameters that are in a file:

eg: input_params.txt is:

    species1.fa species2.fa
    species1.fa species3.fa
    species2.fa species3.fa
    
Then you can run:

    parallel --dry-run --colsep " " "blastp -query {1} -db {2} -out {1/.}.{2/.}.blast" :::: input_params.txt

If you have only one set of inputs (i.e. you are not using ::: or :::: more than once, you can also use this syntax

    cat input_params.txt | parallel --dry-run --colsep " " "blastp -query {1} -db {2} -out {1/.}.{2/.}.blast"


Process a single large file by splitting it up into smaller chunks
---------------------------------------------

All the above examples are great when you want to do the same thing lots of times and want to speed up tasks. What if you want to parallelise the processing of a very large file. For example, if you wanted to blast a set of 1000 sequences against a big database, you might do this:

    split input file into 10 equal sized files
    for each input file
    do
        do something to each input; store each output in a separate file
    done
    concatenate all the output files

eg, in bash:

    split -l 100 input.fasta split_;
    for f in split_??
    do
        blastn -query $f -db nt -out $f.out;
    done
    cat split_??.out >output.txt

or, we could replace the for loop with parallel:

    split -l 100 input.fasta split_;
    parallel "blastn -query {} -db nt -out {}.out" ::: split_??
    cat split_??.out >output.txt

But this has some problems
- we don't know how beforehand how many lines we should split the input into
- we could end up with uneven sized files if all the longest sequences are at the start of the input file. The largest file would be the bottleneck then


There are utilities to get around this (fastasplit from exonerate)

But parallel has a way around that is more elegant and general and applicable to any file format (not just fasta sequences):

Let's build up the command:

    cat input.fasta | parallel --pipe "blastn -query - -db nt" >output.txt
    
We need to tell it to break up the piped file into chunks:

    cat input.fasta | parallel --block 10k --pipe "blastn -query - -db nt" >output.txt

By default it breaks it on a newline closest to the 10k mark, but that might be in the middle of a fasta record, so you use --recstart to specify where a record starts (or recend for where a record ends)

    cat input.fasta | parallel --recstart ">" --block 10k --pipe "blastn -query - -db nt" >output.txt

You can do this for programs like blat as well but it is a bit trickier






Some more tips
---------

Don't forget to put the :::

When you put the main parallel command in "double quotes", special characters like $ inside the command will get interpolated and replaced, so you need to escape special chars, or use 'single quotes'. However, I prefer " " outside and ' ' inside because if your parallel command includes awk/sed/perl one liners as mine do, then it is easier to have the inner awk/sed/perl expressions with ' ' rather than ""

Use --dry-run before you start.

Use --progress

If you are running on a cluster with a shared file system you can do things like

    parallel --sshloginfile logins.txt "command {}" ::: argument_list

where the sshloginfile logins.txt looks something like this (example taken from manual):

    server.example.com
    username@server2.example.com
    8/my-8-core-server.example.com
    2/my_other_username@my-dualcore.example.net
    # This server has SSH running on port 2222
    ssh -p 2222 server.example.net
    4/ssh -p 2222 quadserver.example.net
    # Use a different ssh program
    myssh -p 2222 -l myusername hexacpu.example.net
    # Use a different ssh program with default number of cores
    //usr/local/bin/myssh -p 2222 -l myusername hexacpu
    # Use a different ssh program with 6 cores
    6//usr/local/bin/myssh -p 2222 -l myusername hexacpu
    # Assume 16 cores on the local computer
    16/:
    # Put server1 in hostgroup1
    @hostgroup1/server1
    # Put myusername@server2 in hostgroup1+hostgroup2
    @hostgroup1+hostgroup2/myusername@server2
    # Force 4 cores and put 'ssh -p 2222 server3' in hostgroup1
    @hostgroup1/4/ssh -p 2222 server3
    
Note: ssh keys are essential for this otherwise you will have to enter your password each time!

Things that are heavy on IO will slow down your system if you do many of them in parallel, so be careful.

